<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

[TOC]

# 统计学基本知识梳理

## 二项分布

### 基本描述

二项分布（英语：Binomial distribution）是n个独立的是/非试验中成功的次数的离散概率分布，其中每次试验的成功概率为p。这样的单次成功/失败试验又称为伯努利试验。实际上，当n = 1时，二项分布就是伯努利分布。二项分布是显著性差异的二项试验的基础。

二项分布频繁地用于对以下描述的一种实验进行建模：从总数量大小为N的两个事物中进行n次放回抽样，以某一事物为基准，计算成功抽取这个事物的次数的概率。要注意的是必须进行的是放回抽样，对于不放回抽样我们一般用超几何分布来对这样的实验进行建模。

### 概率质量函数

　　　一般来说，如果一个随机变量X满足二项分布的话，那么它一定有一个参数n∈ ℕ且还有一个参数p∈ [0,1]。这样的话，我们可以把关于X的二项分布写成$X ~ B(n, p)$。对应的概率质量函数如下。

### 累积分布函数

$$
F(x;n,p) = \Pr(X \le x) = \sum_{i=0}^{\lfloor x \rfloor} {n\choose i}p^i(1-p)^{n-i}
$$

## 泊松分布

**Poisson分布**（法语：loi de Poisson，英语：Poisson distribution），译名有**泊松分布**、**普阿松分布**、**帕松分布**、**布瓦松分布**、**布阿松分布**、**波以松分布**、**卜氏分配**等，又称泊松小数法则（Poisson law of small numbers），是一种统计与概率学里常见到的离散概率分布

泊松分布适合于描述单位时间内随机事件发生的次数的概率分布。如某一服务设施在一定时间内受到的服务请求的次数，电话、交换机接到呼叫的次数、汽车站台的候客人数、机器出现的故障数、自然灾害发生的次数、DNA序列的变异数、放射性原子核的衰变数、激光的光子数分布等等。

泊松分布的概率质量函数为：
$$
P(X=k)=\frac{e^{-\lambda}\lambda^k}{k!}
$$

## 大数定律

### 示例

例如，抛掷一颗均匀的6面的骰子，1，2，3，4，5，6应等概率出现，所以每次扔出骰子后，出现点数的期望值是
$$
\frac{1+2+3+4+5+6}{6} = 3.5
$$
根据大数定理，如果多次抛掷骰子，随着抛掷次数的增加，平均值（样本平均值）应该接近3.5，根据大数定理，在多次伯努利实验中，实验概率最后收敛于理论推断的概率值，对于伯努利随机变量，理论推断的成功概率就是期望值，而若对n个相互独立的随机变量的平均值，频率越多则相对越精准。

例如硬币投掷即伯努利实验，当投掷一枚均匀的硬币，理论上得出的正面向上的概率应是1/2。因此，根据大数定理，正面朝上的比例在相对“大”的数字下，“理应”接近为1/2，尤其是正面朝上的概率在n次实验（n接近无限大时）后应几近收敛到1/2。

即使正面朝上（或背面朝上）的比例接近1/2，几乎很自然的正面与负面朝上的绝对差值（absolute difference差值范围）应该相应随着抛掷次数的增加而增加。换句话说，绝对差值的概率应该是会随着抛掷次数而接近于0。直观的来看，绝对差值的期望会增加，只是慢于抛掷次数增加的速度。

### 表现形式

大数定律主要有两种表现形式：'''弱大数定律'''和'''强大数定律'''。定律的两种形式都肯定无疑地表明，样本均值
$$
\overline{X}_n=\frac1n(X_1+\cdots+X_n)
$$
收敛于真值
$$
\overline{X}_n \to \mu \quad\textrm{as}\quad n \to \infty
$$
其中 $X_1$,$X_2$, ... 是独立同分布、期望值$\operatorname{E}(X_1)=\operatorname{E}(X_2)=\,\cdots\,=\mu$ 且皆[[勒贝格可积]]的随机变量构成的无穷序列。$X_j$的勒贝格可积性意味着期望值 $\operatorname{E}(X_j)$存在且有限。

==方差==

$\operatorname{Var}(X_1)=\operatorname{Var}(X_2)=\,\cdots\,= \sigma^2 <\infty$有限的假设是''非必要''的。很大或者无穷大的方差会使其收敛得緩慢一些，但大数定律仍然成立。通常采用这个假设来使证明更加简洁。

### 弱大数定律

'''弱大数定律'''也称为辛钦定理，陈述为：样本均值依概率收敛于期望值。
$$
\overline{X}_n\ \xrightarrow{P}\ \mu \quad\textrm{as}\quad n \to \infty
$$
也就是说对于任意正数 ε,
$$
\lim_{n\to\infty}P\left(\,|\overline{X}_n-\mu| > \varepsilon\,\right) = 0
$$
即
$$
P\left( \lim_{n\to\infty}\overline{X}_n=\mu\right) = 1
$$

### 切比雪夫定理的特殊情况

设$a_1,\ a_2,\ \dots\ ,\ a_n,\ \dots$ 为相互独立的随机变量

数学期望：$\operatorname{E}(a_i) = \mu \quad (i = 1,\ 2,\ \dots)$

方差：$ \operatorname{Var}(a_i) = \sigma^2 \quad (i=1,\ 2,\ \dots) $

则序列： $\overline{a}= \frac{1}{n} \sum_{i=1}^n a_i$ 依概率收敛于$\mu$（即收敛于此数列的数学期望$E(a_i)$）。

换言之，在定理条件下，当 $n$ 无限变大时，$n$个随机变量的算术平均将变成一个常数。

### 伯努利大数定律

设在 $n$ 次独立重复伯努利试验中，
事件$X$发生的次数为$ n_x$。
事件$X$在每次试验中发生的总体概率为$p$。
$ \frac{n_x}{n}$代表样本发生事件$X$的频率。

大数定律可用概率极限值定义:
则对任意正数 $\varepsilon >0 $，下式成立：
$ \lim_{n \to \infty}{P{\left\{ \left|\frac{n_x}{n} - p \right| < \varepsilon \right\}}} = 1 $

定理表明事件发生的频率依概率收敛于事件的总体概率。
定理以严格的数学形式表达了频率的稳定性。
就是说当$n$很大时，事件发生的频率于总体概率率有较大偏差的可能性很小。

## 正态分布

''正态分布''又名“高斯分布''，是一個非常常見的概率分布。正态分布在上十分重要，

若随机变量$X$服從一個位置參數為$\mu$、尺度參數為$\sigma$的正态分布，记为：
$ X \sim N(\mu,\sigma^2) $
則其機率密度函數為
$ f(x) = {1 \over \sigma\sqrt{2\pi} }\,e^{- {{(x-\mu )^2 \over 2\sigma^2}}} $

常態分布的數學期望值或期望值$\mu$等於位置參數，決定了分布的位置；其方差$\sigma^2$的開平方或标准差$\sigma$等於尺度參數，決定了分布的幅度。

通常所說的'''標準常態分布'''是位置參數$\mu = 0$，尺度參數$\sigma^2 = 1$的正态分布。

### 概要

正态分布是自然科学与行为科学中的定量现象的一个方便模型。各种各样的心理学测试分数和物理现象比如光子计数都被发现近似地服从正态分布。尽管这些现象的根本原因经常是未知的，理论上可以证明如果把许多小作用加起来看做一个变量，那么这个变量服从正态分布（在R.N.Bracewell的Fourier transform and its application中可以找到一种简单的证明）。正态分布出现在许多区域统计：例如，采样分布均值是近似地正态的，即使被采样的样本的原始群体分布并不服从正态分布。另外，正态分布信息熵在所有的已知均值及方差的分布中最大，这使得它作为一种均值以及方差已知的分布的自然选择。正态分布是在统计以及许多统计测试中最广泛应用的一类分布。在概率论，正态分布是几种连续以及离散分布的极限分布。

### 正态分布的定义

有几种不同的方法用来说明一个随机变量。最直观的方法是概率密度函数，这种方法能够表示随机变量每个取值有多大的可能性。累积分布函数是一种概率上更加清楚的方法，请看下边的例子。还有一些其他的等价方法，例如cumulant、特征函数、动差生成函数以及cumulant-生成函数。这些方法中有一些对于理论工作非常有用，但是不够直观。请参考关于概率分布的讨论。

==概率密度函數==
'''正态分布'''的概率密度函数均值为$\mu$ 方差为$\sigma^2$或标准差$\sigma$是高斯函數的一个惯例：
$f(x;\mu,\sigma)=\frac{1}{\sigma\sqrt{2\pi}} \, \exp \left( -\frac{(x- \mu)^2}{2\sigma^2} \right) $

请看指數函數以及$\pi$

如果一個隨機變量$X$服從這個分布，我們寫作
$X$ ~ $N(\mu, \sigma^2)$
如果$\mu = 0$並且$\sigma = 1$，這個分布被稱為'''标准正态分布'''，這個分布能夠簡化為

$f(x) = \frac{1}{\sqrt{2\pi}} \, \exp\left(-\frac{x^2}{2} \right)$

右邊是給出了不同參數的正态分布的函數圖。

正态分布中一些值得注意的量：

- 密度函數關於平均值對稱
- 平均值與它的眾數以及中位數（median）同一數值。
- 函數曲線下68.268949%的面積在平均數左右的一個標準差範圍內。
- 95.449974%的面積在平均數左右兩個標準差$2 \sigma$的範圍內。
- 99.730020%的面積在平均數左右三個標準差$3 \sigma$的範圍內。
- 99.993666%的面積在平均數左右四個標準差$4 \sigma$的範圍內。
- 函數曲線的[[拐點]]（inflection point）為離平均數一個標準差距離的位置。

== 累積分布函數 ==

累積分布函數是指隨機變數$X$小於或等於$x$的機率，用機率密度函數表示為
$$
F(x;\mu,\sigma)
=
\frac{1}{\sigma\sqrt{2\pi}}\int_{-\infty}^x \exp \left( -\frac{(t - \mu)^2}{2\sigma^2}\ \right)\, dt.
$$
常態分布的累積分布函数能够由一個叫做误差函数的特殊函数表示：
$$
\Phi(z)=
\frac12 \left[1 + \operatorname{erf}\left(\frac{z-\mu}{\sigma\sqrt2}\right)\right]
$$
标准正态分布的累積分布函數习惯记为$\Phi$，它是指$\mu=0$，$\sigma=1$時的值，
$$
\Phi(x)
=F(x;0,1)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^x\exp\left(-\frac{t^2}{2}\right)\, dt.
$$
將一般常態分布用[[誤差函數]]表示的公式简化，可得：
$$
\Phi(z)=\frac{1}{2} \left[ 1 + \operatorname{erf} \left( \frac{z}{\sqrt{2}} \right) \right]
$$
它的[[反函數]]被稱為反誤差函數，為：
$$
\Phi^{-1}(p)=\sqrt2\;\operatorname{erf}^{-1} \left(2p - 1 \right)
$$
該分位數函數有時也被稱為probit函數。probit函數已被證明沒有初等原函数。

常態分布的分佈函數$\Phi(x)$沒有解析表達式，它的值可以通過[[數值積分]]、[[泰勒級數]]或者[[漸進序列]]近似得到。

正态分布的動差產生函數如下：
$$
M_X(t)={E}\left( e^{tX}\right)
 =\int_{-\infty}^{\infty}\frac{1} {\sigma \sqrt{2\pi} } e^{\left( -\frac{(x - \mu)^2}{2 \sigma^2} \right)} e^{tx}\, dx
 =e^{\left(\mu t + \frac{\sigma^2 t^2}{2}\right)}
$$
過在指數函數內配平方得到。

==特征函数==
特徵函數被定義為$\exp (i t X)$的[[期望值]]，其中$\Phi^{-1}(p)=\sqrt2\;\operatorname{erf}^{-1} \left(2p - 1 \right)i$是虛數單位.对于一个特征函数来讲，特徵函數是：
$$
\phi_X(t;\mu,\sigma)\!=\mathrm{E}\left[ \exp(i t X)\right]
=\int_{-\infty}^{\infty} \frac{1}{\sigma \sqrt{2\pi}} \exp \left(- \frac{(x - \mu)^2}{2\sigma^2} \right) \exp(i t x)\, dx
=\exp\left( i \mu t - \frac{\sigma^2 t^2}{2}\right)
$$
把矩生成函數中的$t$換成$i t<$就能得到特徵函數。

## 置信区间

在统计学中，一个概率样本的**置信区间**（英语：Confidence interval，CI），是对产生这个样本的总体的参数分布（Parametric Distribution）中的某一个未知参数值，以区间形式给出的估计。相对于点估计（Point Estimation）用一个样本统计量来估计参数值，置信区间还蕴含了估计的精确度的信息。在现代机器学习中越来越常用的置信集合（Confidence Set）概念是置信区间在多维分析的推广。

置信区间在频率学派中间使用，其在贝叶斯统计中的对应概念是可信区间（Credible Interval）。两者建立在不同的概念基础上的，贝叶斯统计将分布的位置参数视为随机变量，并对给定观测到的数据之后未知参数的后验分布进行描述，故无论对随机样本还是已观测数据，构造出来的可信区间，其可信水平都是一个合法的概率；而置信区间的置信水平，只在考虑随机样本时可以被理解为一个概率。

### 定义

==对随机样本的定义==
定义置信区间最清晰的方式是从一个'''随机样本'''。考虑一个一维随机变量$ {\cal X} $服从分布$ {\cal F} $，又假设$\theta$是${\cal F}$的参数之一。假设我们的数据采集计划将要独立地抽样$n$次，得到一个随机样本$\{X_1,\ldots,X_n\}$，注意这里所有的$X_i$都是随机的，我们是在讨论一个尚未被观测的数据集。如果存在'''统计量'''(统计量定义为样本$X=\{X_1,\ldots,X_n\}$的一个函数，且不得依赖于任何未知参数)$u(X_1,\ldots,X_n),v(X_1,\ldots,X_n)$满足$u(X_1,\ldots,X_n)<v(X_1,\ldots,X_n)$使得：
$$
\mathbb{P}\left(\theta\in\left(u(X_1,\ldots,X_n),v(X_1,\ldots,X_n)\right)\right)=1-\alpha
$$
则称$ \left(u(X_1,\ldots,X_n),v(X_1,\ldots,X_n)\right)$为一个用于估计参数$\theta$的$1-\alpha$置信区间，其中的$1-\alpha$称为'''置信水平'''。

==对观测到的数据的定义==
接续随机样本版本的定义，现在，对于随机变量${\cal X}$的一个已经观测到的样本$\{x_1,\ldots,x_n\}$，注意这里用小写x表记的$x_i$都是已经观测到的数字，没有随机性了，定义基于数据的$1-\alpha$置信区间为：

 $ \left(u(x_1,\ldots,x_n),v(x_1,\ldots,x_n)\right)$

### 例子

例1：正态分布，已知总体方差$\sigma^2$
$1-\alpha $水平的正态置信区间为
$$
\left( \bar{x}-z_{1-\alpha/2}\frac{\sigma}{\sqrt{n}}, \bar{x}+z_{1-\alpha/2}\frac{\sigma}{\sqrt{n}} \right) $  (双边)\\
\left( -\infty, \bar{x}+z_{1-\alpha}\frac{\sigma}{\sqrt{n}} \right)$  (单边)\\
\left( \bar{x}-z_{1-\alpha}\frac{\sigma}{\sqrt{n}}, +\infty \right)$  (单边)
$$
以下为方便起见，只列出'''双边'''置信区间的例子，且区间中用"$\pm$"进行简记：

例2：正态分布，'''未知'''总体方差$\sigma^2$

$1-\alpha$水平的'''双边'''正态置信区间为：
$\left( \bar{x}\pm t_{n-1;\alpha/2}\frac{s}{\sqrt{n}} \right) $

 例3：两个独立正态样本$x$和$y$，样本大小为$m$和$n$，估计总体均值之差$\mu_1-\mu_2$，假设总体方差'''未知但相等： $\sigma_1=\sigma_2$'''(如果未知且不等就要应用来确定t分布的自由度)
$1-\alpha$水平的'''双边'''正态置信区间为：
 $\left( \bar{x}-\bar{y}\pm t_{m+n-2;\alpha/2}\cdot s_p\cdot \sqrt{\frac1m+\frac1n} \right)$，其中$s_p=\sqrt{\frac{(m-1)s_x^2+(n-1)s_y^2}{m+n-2}}$且$s_x,s_y$分别表示$x$和$y$的样本标准差。

### 构造法

一般来说，置信区间的构造需要先找到一个'''枢轴变量'''（{{Lang|en|Pivotal quantity}}，或称{{Lang|en|Pivot}}），其表达式依赖于样本以及带估计的未知参数(但'''不能'''依赖于总体的其它未知参数)，其分布'''不依赖于'''任何未知参数。

下面以上述例2为例，说明如何利用枢轴变量构造置信区间。对于一个正态分布的随机样本${X_1,\ldots,X_n}$，可以证明(此证明对初学者并不容易)如下统计量'''互相独立'''：
 $\bar{X}=\frac1n \sum_{i=1}^n X_i $  和   $S^2=\frac{\sum_{i=1}^n\left(X_i-\bar{X}\right)^2}{n-1}$
它们的分布是：
 $\frac{\bar{X}-\mu}{\sigma}\sim N(0,1)$  和  $(n-1)\frac{S^2}{\sigma^2} \sim \chi^2_{n-1}$
所以根据[[t分布]]的定义，有
$t = \frac{\bar{X}-\mu}{S/\sqrt{n}}\sim t_{n-1}$
于是反解如下等式左边括号中的不等式
$\mathbb{P}\left( -t_{n-1;\alpha/2}<t=\frac{\bar{X}-\mu}{S\sqrt{n}}<t_{n-1;\alpha/2} \right)=1-\alpha$
就得到了例2中双边置信区间的表达式

### 与参数检验的联系

有时，置信区间可以用来进行参数检验。例如在上面的例1中构造的'''双边'''$1-\alpha$水平置信区间，可以用来检验具有相应的'''显著水平为$\alpha$的双边对立假设，具体地说是如下检验：
正态分布总体，知道总体方差$\sigma^2$，'''在$\alpha$显著水平下'''检验：
 $H_0: \mu=\mu_0$ vs $H_1: \mu \neq\mu_0$
检验方法是：当且仅当相应的$1-\alpha$水平置信区间不包含$\mu_0$时拒绝零假设$H_0$

例1中构造的'''双边'''$1-\alpha$水平置信区间也可以用来检验如下两个显著水平为'''$\alpha/2$'''的'''单边'''对立假设：
 $H_0: \mu\leq \mu_0$ vs $H_1: \mu >\mu_0$
和
 $ H_0: \mu\geq \mu_0$vs $H_1: \mu <\mu_0$
检验方法是完全类似的，比如对于上述第一个单边检验$H_1: \mu >\mu_0$，当且仅当双边置信区间的左端点大于$\mu_0$时拒绝零假设。
