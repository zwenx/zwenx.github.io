# 统计学基本知识梳理

## 二项分布

### 基本描述

二项分布（英语：Binomial distribution）是n个独立的是/非试验中成功的次数的离散概率分布，其中每次试验的成功概率为p。这样的单次成功/失败试验又称为伯努利试验。实际上，当n = 1时，二项分布就是伯努利分布。二项分布是显著性差异的二项试验的基础。

二项分布频繁地用于对以下描述的一种实验进行建模：从总数量大小为N的两个事物中进行n次放回抽样，以某一事物为基准，计算成功抽取这个事物的次数的概率。要注意的是必须进行的是放回抽样，对于不放回抽样我们一般用超几何分布来对这样的实验进行建模。

### 概率质量函数

　　　一般来说，如果一个随机变量X满足二项分布的话，那么它一定有一个参数n∈ ℕ且还有一个参数p∈ [0,1]。这样的话，我们可以把关于X的二项分布写成X ~ B(n, p)。对应的概率质量函数如下。

### 累积分布函数

$$
F(x;n,p) = \Pr(X \le x) = \sum_{i=0}^{\lfloor x \rfloor} {n\choose i}p^i(1-p)^{n-i}
$$

## 泊松分布

**Poisson分布**（法语：loi de Poisson，英语：Poisson distribution），译名有**泊松分布**、**普阿松分布**、**帕松分布**、**布瓦松分布**、**布阿松分布**、**波以松分布**、**卜氏分配**等，又称泊松小数法则（Poisson law of small numbers），是一种统计与概率学里常见到的离散概率分布

泊松分布适合于描述单位时间内随机事件发生的次数的概率分布。如某一服务设施在一定时间内受到的服务请求的次数，电话、交换机接到呼叫的次数、汽车站台的候客人数、机器出现的故障数、自然灾害发生的次数、DNA序列的变异数、放射性原子核的衰变数、激光的光子数分布等等。

泊松分布的概率质量函数为：
$$
P(X=k)=\frac{e^{-\lambda}\lambda^k}{k!}
$$


## 大数定律

### 示例

例如，抛掷一颗均匀的6面的骰子，1，2，3，4，5，6应等概率出现，所以每次扔出骰子后，出现点数的期望值是
$$
\frac{1+2+3+4+5+6}{6} = 3.5
$$


根据大数定理，如果多次抛掷骰子，随着抛掷次数的增加，平均值（样本平均值）应该接近3.5，根据大数定理，在多次伯努利实验中，实验概率最后收敛于理论推断的概率值，对于伯努利随机变量，理论推断的成功概率就是期望值，而若对n个相互独立的随机变量的平均值，频率越多则相对越精准。

例如硬币投掷即伯努利实验，当投掷一枚均匀的硬币，理论上得出的正面向上的概率应是1/2。因此，根据大数定理，正面朝上的比例在相对“大”的数字下，“理应”接近为1/2，尤其是正面朝上的概率在n次实验（n接近无限大时）后应几近收敛到1/2。

即使正面朝上（或背面朝上）的比例接近1/2，几乎很自然的正面与负面朝上的绝对差值（absolute difference差值范围）应该相应随着抛掷次数的增加而增加。换句话说，绝对差值的概率应该是会随着抛掷次数而接近于0。直观的来看，绝对差值的期望会增加，只是慢于抛掷次数增加的速度。

### 表现形式

大数定律主要有两种表现形式：'''弱大数定律'''和'''强大数定律'''。定律的两种形式都肯定无疑地表明，样本均值
$$
\overline{X}_n=\frac1n(X_1+\cdots+X_n)
$$
收敛于真值
$$
\overline{X}_n \to \mu \quad\textrm{as}\quad n \to \infty
$$
其中 $X_1$,$X_2$, ... 是独立同分布、期望值$\operatorname{E}(X_1)=\operatorname{E}(X_2)=\,\cdots\,=\mu$ 且皆[[勒贝格可积]]的随机变量构成的无穷序列。$X_j$的勒贝格可积性意味着期望值 $\operatorname{E}(X_j)$存在且有限。

#### 方差

$\operatorname{Var}(X_1)=\operatorname{Var}(X_2)=\,\cdots\,= \sigma^2 <\infty$有限的假设是'''非必要'''的。很大或者无穷大的方差会使其收敛得緩慢一些，但大数定律仍然成立。通常采用这个假设来使证明更加简洁。

### 弱大数定律

'''弱大数定律'''也称为辛钦定理，陈述为：样本均值[[依概率收敛]]于期望值。
$$
\overline{X}_n\ \xrightarrow{P}\ \mu \quad\textrm{as}\quad n \to \infty
$$
也就是说对于任意正数 ''ε'',
$$
\lim_{n\to\infty}P\left(\,|\overline{X}_n-\mu| > \varepsilon\,\right) = 0
$$
即
$$
P\left( \lim_{n\to\infty}\overline{X}_n=\mu\right) = 1
$$

### 切比雪夫定理的特殊情况

设$a_1,\ a_2,\ \dots\ ,\ a_n,\ \dots$ 为相互独立的随机变量，其[[数学期望]]为：$ \operatorname{E}(a_i) = \mu \quad (i = 1,\ 2,\ \dots) $，[[方差]]为：$ \operatorname{Var}(a_i) = \sigma^2 \quad (i=1,\ 2,\ \dots)$

则序列$\overline{a}= \frac{1}{n} \sum_{i=1}^n a_i</math>[[依概率收敛]]于<math>\mu</math>（即收敛于此数列的数学期望<math>E(a_i)$）。

换言之，在定理条件下，当$n$无限变大时，$n$个随机变量的[[算术平均]]将变成一个常数。

### 伯努利大数定律

设在$n$次独立重复[[伯努利试验]]中，
事件$X$发生的次数为$ n_x$。
事件$X$在每次试验中发生的母體機率为$p$。
$ \frac{n_x}{n}$代表样本发生事件$X$的频率。

大数定律可用概率极限值定义:
则对任意正数$\varepsilon >0 $，下式成立：
:$ \lim_{n \to \infty}{P{\left\{ \left|\frac{n_x}{n} - p \right| < \varepsilon \right\}}} = 1 $

定理表明事件发生的频率依概率收敛于事件的总体概率。
定理以严格的数学形式表达了频率的稳定性。
就是说当$n$很大时，事件发生的频率于总体概率率有较大偏差的可能性很小。



## 正态分布

''正态分布''又名“高斯分布''，是一個非常常見的概率分布。正态分布在上十分重要，

若随机变量$X$服從一個位置參數為$\mu$、尺度參數為$\sigma$的正态分布，记为：
￥X \sim N(\mu,\sigma^2)$
則其機率密度函數為
$f(x) = {1 \over \sigma\sqrt{2\pi} }\,e^{- {{(x-\mu )^2 \over 2\sigma^2}}}$

常態分布的[[數學期望]]值或[[期望值]]<math>\mu</math>等於位置參數，決定了分布的位置；其[[方差]]$\sigma^2$的開平方或[[標準差]]$\sigma$等於尺度參數，決定了分布的幅度。

常態分布的機率密度函數曲線呈鐘形，因此人們又經常稱之為'''鐘形曲線'''（类似于寺庙里的大钟，因此得名）。我們通常所說的'''標準常態分布'''是位置參數$\mu = 0$，尺度參數$\sigma^2 = 1$的正态分布。

### 
